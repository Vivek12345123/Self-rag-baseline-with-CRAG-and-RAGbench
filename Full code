from vllm import LLM, SamplingParams
import json
import time
import os
import subprocess
import sys
from typing import List, Dict, Any, Optional
import logging
from datasets import load_dataset
import numpy as np
import bz2
import tarfile
import requests
from urllib.parse import urljoin
import kagglehub

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SelfRAGModel:
    """Complete Self-RAG model implementation following the original README"""
    
    def __init__(self, 
                 model_path: str = "selfrag/selfrag_llama2_7b",
                 download_dir: str = "/gscratch/h2lab/akari/model_cache",
                 dtype: str = "half"):
        """Initialize Self-RAG model exactly as in README"""
        self.model = LLM(model_path, download_dir=download_dir, dtype=dtype)
        self.sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, skip_special_tokens=False)

    def format_prompt(self, input, paragraph=None):
        """Format prompt exactly as in README"""
        prompt = "### Instruction:\n{0}\n\n### Response:\n".format(input)
        if paragraph is not None:
            prompt += "[Retrieval]<paragraph>{0}</paragraph>".format(paragraph)
        return prompt

    def extract_utility_score(self, text: str) -> int:
        """Extract utility score from Self-RAG output"""
        for i in range(5, 0, -1):
            if f'[Utility:{i}]' in text:
                return i
        return 0

def benchmark_selfrag(model, queries: List[str], contexts: List[str] = None) -> List[Dict[str, Any]]:
    """
    Original benchmark function from the provided code
    
    Args:
        model: SelfRAG model instance
        queries: List of input queries
        contexts: Optional retrieved contexts (if None, runs without retrieval)
    
    Returns:
        List of results with timing and response data
    """
    results = []
    
    if contexts is None:
        # No retrieval mode
        prompts = [model.format_prompt(query) for query in queries]
        
        start_time = time.time()
        preds = model.model.generate(prompts, model.sampling_params)
        inference_time = time.time() - start_time
        
        for i, (query, pred) in enumerate(zip(queries, preds)):
            results.append({
                'query': query,
                'response': pred.outputs[0].text,
                'mode': 'no_retrieval',
                'inference_time': inference_time / len(queries),
                'tokens_generated': len(pred.outputs[0].token_ids),
                'retrieval_used': '[Retrieve]' in pred.outputs[0].text
            })
    else:
        # With retrieval mode
        prompts = [model.format_prompt(query, context) for query, context in zip(queries, contexts)]
        
        start_time = time.time()
        preds = model.model.generate(prompts, model.sampling_params)
        inference_time = time.time() - start_time
        
        for i, (query, context, pred) in enumerate(zip(queries, contexts, preds)):
            response_text = pred.outputs[0].text
            results.append({
                'query': query,
                'context': context,
                'response': response_text,
                'mode': 'with_retrieval',
                'inference_time': inference_time / len(queries),
                'tokens_generated': len(pred.outputs[0].token_ids),
                'is_relevant': '[Relevant]' in response_text,
                'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                'utility_score': model.extract_utility_score(response_text)
            })
    
    return results

def download_crag_data():
    """Download CRAG data following the official documentation"""
    data_dir = "crag_data"
    os.makedirs(data_dir, exist_ok=True)
    
    # CRAG data URLs from the documentation
    crag_urls = {
        "task_1_and_2": "https://github.com/facebookresearch/CRAG/raw/refs/heads/main/data/crag_task_1_and_2_dev_v4.jsonl.bz2",
        "task_3_part1": "https://github.com/facebookresearch/CRAG/raw/refs/heads/main/data/crag_task_3_dev_v4.tar.bz2.part1",
        "task_3_part2": "https://github.com/facebookresearch/CRAG/raw/refs/heads/main/data/crag_task_3_dev_v4.tar.bz2.part2", 
        "task_3_part3": "https://github.com/facebookresearch/CRAG/raw/refs/heads/main/data/crag_task_3_dev_v4.tar.bz2.part3",
        "task_3_part4": "https://github.com/facebookresearch/CRAG/raw/refs/heads/main/data/crag_task_3_dev_v4.tar.bz2.part4"
    }
    
    # Download Task 1 and 2 data
    task_1_2_file = os.path.join(data_dir, "crag_task_1_and_2_dev_v4.jsonl.bz2")
    if not os.path.exists(task_1_2_file):
        logger.info("Downloading CRAG Task 1 and 2 data...")
        try:
            response = requests.get(crag_urls["task_1_and_2"], stream=True)
            response.raise_for_status()
            with open(task_1_2_file, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            logger.info("CRAG Task 1 and 2 data downloaded successfully")
        except Exception as e:
            logger.error(f"Failed to download CRAG Task 1 and 2 data: {e}")
            return None
    
    return data_dir

def load_crag_data(data_dir: str, task: str = "1_and_2", sample_size: int = 100):
    """Load CRAG data from downloaded files following the official documentation"""
    
    if task == "1_and_2":
        file_path = os.path.join(data_dir, "crag_task_1_and_2_dev_v4.jsonl.bz2")
        
        if not os.path.exists(file_path):
            logger.error(f"CRAG data file not found: {file_path}")
            return None
        
        try:
            logger.info(f"Loading CRAG Task {task} data from {file_path}")
            data = []
            
            with bz2.open(file_path, 'rt', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= sample_size:
                        break
                    try:
                        item = json.loads(line.strip())
                        data.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Skipping invalid JSON line {i}: {e}")
                        continue
            
            logger.info(f"Loaded {len(data)} CRAG samples")
            return data
            
        except Exception as e:
            logger.error(f"Error loading CRAG data: {e}")
            return None
    
    else:
        logger.error(f"Task {task} not implemented yet. Only '1_and_2' is currently supported.")
        return None

def run_crag_benchmark(model, task: str = "1_and_2", sample_size: int = 100):
    """
    CRAG Benchmark Implementation following official documentation
    
    Args:
        model: SelfRAG model instance
        task: CRAG task ("1_and_2" or "3")
        sample_size: Number of samples to evaluate
    """
    logger.info(f"Setting up CRAG Task {task} benchmark...")
    
    # Download CRAG data
    data_dir = download_crag_data()
    if data_dir is None:
        logger.error("Failed to download CRAG data")
        return []
    
    # Load CRAG data
    crag_data = load_crag_data(data_dir, task, sample_size)
    if crag_data is None:
        logger.error("Failed to load CRAG dataset")
        return []
    
    logger.info(f"Running CRAG Task {task} benchmark with {len(crag_data)} samples...")
    
    crag_results = []
    for i, item in enumerate(crag_data):
        try:
            # Extract data following CRAG schema
            interaction_id = item.get('interaction_id', f'crag_{i}')
            query = item.get('query', '')
            answer = item.get('answer', '')
            alt_ans = item.get('alt_ans', [])
            domain = item.get('domain', 'unknown')
            question_type = item.get('question_type', 'unknown')
            search_results = item.get('search_results', [])
            
            # For Task 1 & 2: use first 5 search results (as per documentation)
            context = None
            if search_results and task == "1_and_2":
                # Use page snippets as context (5 pages randomly selected from top-10)
                contexts = []
                for result in search_results[:5]:  # Take first 5 as they're already randomly selected
                    page_snippet = result.get('page_snippet', '')
                    page_name = result.get('page_name', '')
                    if page_snippet:
                        contexts.append(f"{page_name}: {page_snippet}")
                
                if contexts:
                    context = "\n".join(contexts)
            
            # Generate Self-RAG response
            if context:
                prompt = model.format_prompt(query, context)
            else:
                prompt = model.format_prompt(query)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # CRAG evaluation - exact match with answer or alternative answers
            exact_match = False
            if answer:
                exact_match = answer.lower().strip() in response_text.lower()
            
            # Check alternative answers
            if not exact_match and alt_ans:
                for alt in alt_ans:
                    if alt and alt.lower().strip() in response_text.lower():
                        exact_match = True
                        break
            
            crag_results.append({
                'interaction_id': interaction_id,
                'query': query,
                'context': context,
                'response': response_text,
                'ground_truth': answer,
                'alt_answers': alt_ans,
                'domain': domain,
                'question_type': question_type,
                'exact_match': exact_match,
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'is_relevant': '[Relevant]' in response_text,
                'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                'utility_score': model.extract_utility_score(response_text),
                'num_search_results': len(search_results)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(crag_data)} CRAG samples")
            
        except Exception as e:
            logger.error(f"Error processing CRAG item {i}: {e}")
            continue
    
    if crag_results:
        avg_exact_match = np.mean([r['exact_match'] for r in crag_results])
        avg_utility = np.mean([r['utility_score'] for r in crag_results])
        relevance_rate = np.mean([r['is_relevant'] for r in crag_results])
        support_rate = np.mean([r['is_supported'] for r in crag_results])
        
        logger.info(f"CRAG Task {task} benchmark completed:")
        logger.info(f"  Exact Match: {avg_exact_match:.3f}")
        logger.info(f"  Average Utility: {avg_utility:.3f}")
        logger.info(f"  Relevance Rate: {relevance_rate:.3f}")
        logger.info(f"  Support Rate: {support_rate:.3f}")
    
    return crag_results

def run_ragbench_benchmark(model, dataset_name: str = "covidqa", sample_size: int = 100):
    """
    RAGBench Implementation using HuggingFace datasets
    Supports: covidqa, cuad, delucionqa
    """
    logger.info(f"Running RAGBench benchmark on {dataset_name} with {sample_size} samples...")
    
    try:
        # Load RAGBench dataset with error handling for different dataset structures
        dataset_path = f"galileo-ai/ragbench"
        
        # Try loading the dataset with the specific subset
        try:
            ds = load_dataset(dataset_path, dataset_name)
        except:
            # If that fails, try without the subset name
            logger.info(f"Trying to load {dataset_path} without subset...")
            ds = load_dataset(dataset_path)
        
        # Determine which split to use
        available_splits = list(ds.keys())
        logger.info(f"Available splits: {available_splits}")
        
        if 'test' in available_splits:
            dataset = ds['test']
        elif 'validation' in available_splits:
            dataset = ds['validation']
        elif 'train' in available_splits:
            dataset = ds['train']
        else:
            # If no standard splits, use the first available
            dataset = ds[available_splits[0]]
        
        # Take sample
        if sample_size < len(dataset):
            dataset = dataset.select(range(sample_size))
        
        logger.info(f"Using {len(dataset)} samples from RAGBench {dataset_name}")
        
        ragbench_results = []
        
        for i, item in enumerate(dataset):
            try:
                # Handle different possible field names
                query = item.get('question', item.get('query', item.get('input', f"Question {i+1}")))
                context = item.get('context', item.get('passage', item.get('document', None)))
                ground_truth = item.get('answer', item.get('target', item.get('ground_truth', 'Unknown')))
                
                # Generate Self-RAG response
                if context and context.strip():
                    prompt = model.format_prompt(query, context)
                else:
                    prompt = model.format_prompt(query)
                
                start_time = time.time()
                pred = model.model.generate([prompt], model.sampling_params)[0]
                inference_time = time.time() - start_time
                
                response_text = pred.outputs[0].text
                
                # RAGBench metrics
                exact_match = ground_truth.lower().strip() in response_text.lower() if ground_truth and ground_truth != 'Unknown' else False
                
                ragbench_results.append({
                    'dataset': dataset_name,
                    'query': query,
                    'context': context,
                    'response': response_text,
                    'ground_truth': ground_truth,
                    'exact_match': exact_match,
                    'inference_time': inference_time,
                    'tokens_generated': len(pred.outputs[0].token_ids),
                    'is_relevant': '[Relevant]' in response_text,
                    'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                    'utility_score': model.extract_utility_score(response_text)
                })
                
            except Exception as e:
                logger.error(f"Error processing RAGBench item {i}: {e}")
                continue
        
        if ragbench_results:
            avg_em = np.mean([r['exact_match'] for r in ragbench_results])
            logger.info(f"RAGBench {dataset_name} completed. Exact Match: {avg_em:.3f}")
        
        return ragbench_results
        
    except Exception as e:
        logger.error(f"Error running RAGBench {dataset_name}: {e}")
        logger.info("Please ensure you have access to the dataset. You may need: huggingface-cli login")
        return []

def run_all_ragbench_datasets(model, sample_size: int = 100):
    """Run all available RAGBench datasets"""
    all_results = {}
    
    # Try different possible dataset names
    possible_datasets = ["covidqa", "cuad", "delucionqa"]
    
    # First, try to discover what datasets are actually available
    try:
        logger.info("Discovering available RAGBench datasets...")
        # This would need to be adapted based on the actual RAGBench structure
        ds_info = load_dataset("galileo-ai/ragbench", split="train", streaming=True)
        logger.info(f"RAGBench dataset loaded successfully")
    except Exception as e:
        logger.warning(f"Could not load RAGBench info: {e}")
    
    for dataset_name in possible_datasets:
        logger.info(f"Attempting to run {dataset_name}...")
        results = run_ragbench_benchmark(model, dataset_name, sample_size)
        if results:  # Only add if we got results
            all_results[dataset_name] = results
        else:
            logger.warning(f"No results obtained for {dataset_name}")
    
    return all_results

def download_squad_data():
    """Download SQuAD dataset using kagglehub"""
    logger.info("Downloading SQuAD dataset...")
    try:
        # Download latest version
        path = kagglehub.dataset_download("stanfordu/stanford-question-answering-dataset")
        logger.info(f"Path to SQuAD dataset files: {path}")
        return path
    except Exception as e:
        logger.error(f"Error downloading SQuAD dataset: {e}")
        logger.info("Please ensure kagglehub is installed: pip install kagglehub")
        return None

def run_squad_v2_benchmark(model, sample_size: int = 100):
    """
    SQuAD v2 Benchmark Implementation using downloaded data
    
    Args:
        model: SelfRAG model instance
        sample_size: Number of samples to evaluate
    """
    logger.info(f"Running SQuAD v2 benchmark with {sample_size} samples...")
    
    # First try to download using kagglehub
    data_path = download_squad_data()
    
    # Fallback to HuggingFace if kagglehub fails
    if data_path is None:
        logger.warning("Kagglehub download failed, trying HuggingFace datasets...")
        try:
            ds = load_dataset("squad_v2")
            
            # Use validation split for evaluation
            if 'validation' in ds:
                dataset = ds['validation']
            else:
                dataset = ds['train']
            
            # Take sample
            if sample_size < len(dataset):
                dataset = dataset.select(range(sample_size))
            
            logger.info(f"Using {len(dataset)} samples from SQuAD v2 via HuggingFace")
            
            squad_results = []
            
            for i, item in enumerate(dataset):
                try:
                    # Extract SQuAD v2 fields
                    question = item.get('question', '')
                    context = item.get('context', '')
                    answers = item.get('answers', {})
                    is_impossible = item.get('is_impossible', False)
                    squad_id = item.get('id', f'squad_{i}')
                    
                    # Get answer texts
                    answer_texts = answers.get('text', []) if answers else []
                    
                    # Generate Self-RAG response
                    if context.strip():
                        prompt = model.format_prompt(question, context)
                    else:
                        prompt = model.format_prompt(question)
                    
                    start_time = time.time()
                    pred = model.model.generate([prompt], model.sampling_params)[0]
                    inference_time = time.time() - start_time
                    
                    response_text = pred.outputs[0].text
                    
                    # SQuAD v2 evaluation
                    exact_match = False
                    if not is_impossible and answer_texts:
                        for ans_text in answer_texts:
                            if ans_text and ans_text.lower().strip() in response_text.lower():
                                exact_match = True
                                break
                    elif is_impossible:
                        # For impossible questions, check if model indicates no answer
                        no_answer_indicators = ["no answer", "cannot answer", "not provided", "unknown"]
                        exact_match = any(indicator in response_text.lower() for indicator in no_answer_indicators)
                    
                    squad_results.append({
                        'id': squad_id,
                        'question': question,
                        'context': context,
                        'response': response_text,
                        'ground_truth_answers': answer_texts,
                        'is_impossible': is_impossible,
                        'exact_match': exact_match,
                        'inference_time': inference_time,
                        'tokens_generated': len(pred.outputs[0].token_ids),
                        'is_relevant': '[Relevant]' in response_text,
                        'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                        'utility_score': model.extract_utility_score(response_text)
                    })
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"Processed {i + 1}/{len(dataset)} SQuAD v2 samples")
                        
                except Exception as e:
                    logger.error(f"Error processing SQuAD v2 item {i}: {e}")
                    continue
            
            if squad_results:
                avg_em = np.mean([r['exact_match'] for r in squad_results])
                avg_utility = np.mean([r['utility_score'] for r in squad_results])
                relevance_rate = np.mean([r['is_relevant'] for r in squad_results])
                support_rate = np.mean([r['is_supported'] for r in squad_results])
                impossible_count = sum([r['is_impossible'] for r in squad_results])
                
                logger.info(f"SQuAD v2 benchmark completed:")
                logger.info(f"  Exact Match: {avg_em:.3f}")
                logger.info(f"  Average Utility: {avg_utility:.3f}")
                logger.info(f"  Relevance Rate: {relevance_rate:.3f}")
                logger.info(f"  Support Rate: {support_rate:.3f}")
                logger.info(f"  Impossible Questions: {impossible_count}/{len(squad_results)}")
            
            return squad_results
            
        except Exception as e:
            logger.error(f"Error loading SQuAD v2 from HuggingFace: {e}")
            return []
    
    else:
        # Process downloaded kagglehub data
        logger.info(f"Processing SQuAD data from {data_path}")
        
        # Look for JSON files in the downloaded directory
        import glob
        json_files = glob.glob(os.path.join(data_path, "*.json"))
        
        if not json_files:
            logger.error("No JSON files found in downloaded SQuAD data")
            return []
        
        squad_results = []
        processed_count = 0
        
        for json_file in json_files:
            if processed_count >= sample_size:
                break
                
            logger.info(f"Processing {json_file}")
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    squad_data = json.load(f)
                
                # Navigate SQuAD JSON structure
                for article in squad_data.get('data', []):
                    if processed_count >= sample_size:
                        break
                        
                    for paragraph in article.get('paragraphs', []):
                        if processed_count >= sample_size:
                            break
                            
                        context = paragraph.get('context', '')
                        
                        for qa in paragraph.get('qas', []):
                            if processed_count >= sample_size:
                                break
                                
                            try:
                                question = qa.get('question', '')
                                answers = qa.get('answers', [])
                                is_impossible = qa.get('is_impossible', False)
                                squad_id = qa.get('id', f'squad_{processed_count}')
                                
                                # Extract answer texts
                                answer_texts = [ans.get('text', '') for ans in answers if ans.get('text')]
                                
                                # Generate Self-RAG response
                                if context.strip():
                                    prompt = model.format_prompt(question, context)
                                else:
                                    prompt = model.format_prompt(question)
                                
                                start_time = time.time()
                                pred = model.model.generate([prompt], model.sampling_params)[0]
                                inference_time = time.time() - start_time
                                
                                response_text = pred.outputs[0].text
                                
                                # SQuAD v2 evaluation
                                exact_match = False
                                if not is_impossible and answer_texts:
                                    for ans_text in answer_texts:
                                        if ans_text and ans_text.lower().strip() in response_text.lower():
                                            exact_match = True
                                            break
                                elif is_impossible:
                                    # For impossible questions, check if model indicates no answer
                                    no_answer_indicators = ["no answer", "cannot answer", "not provided", "unknown"]
                                    exact_match = any(indicator in response_text.lower() for indicator in no_answer_indicators)
                                
                                squad_results.append({
                                    'id': squad_id,
                                    'question': question,
                                    'context': context,
                                    'response': response_text,
                                    'ground_truth_answers': answer_texts,
                                    'is_impossible': is_impossible,
                                    'exact_match': exact_match,
                                    'inference_time': inference_time,
                                    'tokens_generated': len(pred.outputs[0].token_ids),
                                    'is_relevant': '[Relevant]' in response_text,
                                    'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                                    'utility_score': model.extract_utility_score(response_text)
                                })
                                
                                processed_count += 1
                                
                                if processed_count % 10 == 0:
                                    logger.info(f"Processed {processed_count} SQuAD v2 samples")
                                    
                            except Exception as e:
                                logger.error(f"Error processing SQuAD QA item: {e}")
                                continue
                                
            except Exception as e:
                logger.error(f"Error reading SQuAD JSON file {json_file}: {e}")
                continue
        
        if squad_results:
            avg_em = np.mean([r['exact_match'] for r in squad_results])
            avg_utility = np.mean([r['utility_score'] for r in squad_results])
            relevance_rate = np.mean([r['is_relevant'] for r in squad_results])
            support_rate = np.mean([r['is_supported'] for r in squad_results])
            impossible_count = sum([r['is_impossible'] for r in squad_results])
            
            logger.info(f"SQuAD v2 benchmark completed:")
            logger.info(f"  Exact Match: {avg_em:.3f}")
            logger.info(f"  Average Utility: {avg_utility:.3f}")
            logger.info(f"  Relevance Rate: {relevance_rate:.3f}")
            logger.info(f"  Support Rate: {support_rate:.3f}")
            logger.info(f"  Impossible Questions: {impossible_count}/{len(squad_results)}")
        
        return squad_results

def download_natural_questions_data():
    """Download Natural Questions data using gsutil"""
    data_dir = "natural_questions_data"
    os.makedirs(data_dir, exist_ok=True)
    
    logger.info("Downloading Natural Questions data...")
    try:
        # Check if gsutil is available
        subprocess.run(["gsutil", "--version"], check=True, capture_output=True)
        
        # Download the data
        cmd = ["gsutil", "-m", "cp", "-R", "gs://natural_questions/v1.0", data_dir]
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        logger.info("Natural Questions data downloaded successfully")
        return data_dir
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Error downloading Natural Questions data: {e}")
        logger.error(f"Please ensure gsutil is installed and configured")
        return None
    except FileNotFoundError:
        logger.error("gsutil not found. Please install Google Cloud SDK")
        return None

def run_natural_questions_benchmark(model, sample_size: int = 100):
    """
    Natural Questions Benchmark Implementation
    
    Args:
        model: SelfRAG model instance
        sample_size: Number of samples to evaluate
    """
    logger.info(f"Running Natural Questions benchmark with {sample_size} samples...")
    
    # Try to download the data first
    data_dir = download_natural_questions_data()
    if data_dir is None:
        logger.warning("Could not download Natural Questions data via gsutil, trying HuggingFace...")
        
        # Fallback to HuggingFace if available
        try:
            ds = load_dataset("natural_questions", split="train")
            if sample_size < len(ds):
                ds = ds.select(range(sample_size))
            
            logger.info(f"Using {len(ds)} samples from Natural Questions via HuggingFace")
            
            nq_results = []
            
            for i, item in enumerate(ds):
                try:
                    # Extract Natural Questions fields
                    question_text = item.get('question', {}).get('text', '')
                    document_html = item.get('document', {}).get('html', '')
                    annotations = item.get('annotations', [])
                    
                    # Get short answers from annotations
                    short_answers = []
                    long_answers = []
                    for annotation in annotations:
                        if annotation.get('short_answers'):
                            for sa in annotation['short_answers']:
                                if sa.get('text'):
                                    short_answers.append(sa['text'])
                        if annotation.get('long_answer', {}).get('candidate_text'):
                            long_answers.append(annotation['long_answer']['candidate_text'])
                    
                    # Use document HTML as context (truncated)
                    context = document_html[:2000] if document_html else None  # Truncate for efficiency
                    
                    # Generate Self-RAG response
                    if context:
                        prompt = model.format_prompt(question_text, context)
                    else:
                        prompt = model.format_prompt(question_text)
                    
                    start_time = time.time()
                    pred = model.model.generate([prompt], model.sampling_params)[0]
                    inference_time = time.time() - start_time
                    
                    response_text = pred.outputs[0].text
                    
                    # Natural Questions evaluation - check if any short answer is in response
                    exact_match = False
                    if short_answers:
                        for ans in short_answers:
                            if ans and ans.lower().strip() in response_text.lower():
                                exact_match = True
                                break
                    
                    nq_results.append({
                        'question': question_text,
                        'context': context,
                        'response': response_text,
                        'short_answers': short_answers,
                        'long_answers': long_answers,
                        'exact_match': exact_match,
                        'inference_time': inference_time,
                        'tokens_generated': len(pred.outputs[0].token_ids),
                        'is_relevant': '[Relevant]' in response_text,
                        'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                        'utility_score': model.extract_utility_score(response_text)
                    })
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"Processed {i + 1}/{len(ds)} Natural Questions samples")
                        
                except Exception as e:
                    logger.error(f"Error processing Natural Questions item {i}: {e}")
                    continue
            
            if nq_results:
                avg_em = np.mean([r['exact_match'] for r in nq_results])
                avg_utility = np.mean([r['utility_score'] for r in nq_results])
                relevance_rate = np.mean([r['is_relevant'] for r in nq_results])
                support_rate = np.mean([r['is_supported'] for r in nq_results])
                
                logger.info(f"Natural Questions benchmark completed:")
                logger.info(f"  Exact Match: {avg_em:.3f}")
                logger.info(f"  Average Utility: {avg_utility:.3f}")
                logger.info(f"  Relevance Rate: {relevance_rate:.3f}")
                logger.info(f"  Support Rate: {support_rate:.3f}")
            
            return nq_results
            
        except Exception as e:
            logger.error(f"Error loading Natural Questions from HuggingFace: {e}")
            logger.info("Please ensure you have access to the dataset or gsutil configured")
            return []
    
    # If gsutil download worked, process the downloaded files
    # This would need additional implementation based on the actual file structure
    logger.info("Processing downloaded Natural Questions files...")
    # Implementation would depend on the actual file structure from gsutil
    return []

def run_hotpot_qa_benchmark(model, variant: str = "distractor", sample_size: int = 100):
    """
    HotpotQA Benchmark Implementation
    
    Args:
        model: SelfRAG model instance
        variant: HotpotQA variant ("distractor" or "fullwiki")
        sample_size: Number of samples to evaluate
    """
    logger.info(f"Running HotpotQA {variant} benchmark with {sample_size} samples...")
    
    try:
        # Load HotpotQA dataset
        ds = load_dataset("hotpotqa/hotpot_qa", variant)
        
        # Determine which split to use
        available_splits = list(ds.keys())
        logger.info(f"Available splits: {available_splits}")
        
        if 'validation' in available_splits:
            dataset = ds['validation']
        elif 'train' in available_splits:
            dataset = ds['train']
        else:
            dataset = ds[available_splits[0]]
        
        # Take sample
        if sample_size < len(dataset):
            dataset = dataset.select(range(sample_size))
        
        logger.info(f"Using {len(dataset)} samples from HotpotQA {variant}")
        
        hotpot_results = []
        
        for i, item in enumerate(dataset):
            try:
                # Extract HotpotQA fields
                question = item.get('question', '')
                answer = item.get('answer', '')
                context = item.get('context', [])
                supporting_facts = item.get('supporting_facts', [])
                level = item.get('level', 'unknown')
                type_question = item.get('type', 'unknown')
                
                # Build context from supporting paragraphs
                context_text = ""
                if context:
                    context_paragraphs = []
                    for title, sentences in context:
                        if sentences:  # Only use non-empty paragraphs
                            paragraph_text = f"{title}: {' '.join(sentences)}"
                            context_paragraphs.append(paragraph_text)
                    
                    if context_paragraphs:
                        context_text = "\n".join(context_paragraphs[:5])  # Use first 5 paragraphs
                
                # Generate Self-RAG response
                if context_text.strip():
                    prompt = model.format_prompt(question, context_text)
                else:
                    prompt = model.format_prompt(question)
                
                start_time = time.time()
                pred = model.model.generate([prompt], model.sampling_params)[0]
                inference_time = time.time() - start_time
                
                response_text = pred.outputs[0].text
                
                # HotpotQA evaluation - exact match with answer
                exact_match = answer.lower().strip() in response_text.lower() if answer else False
                
                hotpot_results.append({
                    'variant': variant,
                    'question': question,
                    'context': context_text,
                    'response': response_text,
                    'ground_truth': answer,
                    'supporting_facts': supporting_facts,
                    'level': level,
                    'type': type_question,
                    'exact_match': exact_match,
                    'inference_time': inference_time,
                    'tokens_generated': len(pred.outputs[0].token_ids),
                    'is_relevant': '[Relevant]' in response_text,
                    'is_supported': '[Fully supported]' in response_text or '[Partially supported]' in response_text,
                    'utility_score': model.extract_utility_score(response_text),
                    'num_context_paragraphs': len(context) if context else 0
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(dataset)} HotpotQA {variant} samples")
                    
            except Exception as e:
                logger.error(f"Error processing HotpotQA item {i}: {e}")
                continue
        
        if hotpot_results:
            avg_em = np.mean([r['exact_match'] for r in hotpot_results])
            avg_utility = np.mean([r['utility_score'] for r in hotpot_results])
            relevance_rate = np.mean([r['is_relevant'] for r in hotpot_results])
            support_rate = np.mean([r['is_supported'] for r in hotpot_results])
            
            logger.info(f"HotpotQA {variant} benchmark completed:")
            logger.info(f"  Exact Match: {avg_em:.3f}")
            logger.info(f"  Average Utility: {avg_utility:.3f}")
            logger.info(f"  Relevance Rate: {relevance_rate:.3f}")
            logger.info(f"  Support Rate: {support_rate:.3f}")
        
        return hotpot_results
        
    except Exception as e:
        logger.error(f"Error running HotpotQA {variant}: {e}")
        logger.info("Please ensure you have access to the dataset. You may need: huggingface-cli login")
        return []

def run_all_hotpot_variants(model, sample_size: int = 100):
    """Run both HotpotQA variants"""
    all_results = {}
    
    variants = ["distractor", "fullwiki"]
    
    for variant in variants:
        logger.info(f"Running HotpotQA {variant}...")
        results = run_hotpot_qa_benchmark(model, variant, sample_size)
        if results:
            all_results[f"hotpot_{variant}"] = results
        else:
            logger.warning(f"No results obtained for HotpotQA {variant}")
    
    return all_results

def main():
    """Main function to run Self-RAG benchmarks"""
    print("Initializing Self-RAG Model...")
    
    # Initialize model exactly as in README
    try:
        model = SelfRAGModel()
        print("Self-RAG Model loaded successfully!")
    except Exception as e:
        print(f"Error loading model: {e}")
        print("Please ensure vLLM and the model are properly installed")
        return
    
    # Original benchmark from the provided code
    print("\n" + "="*50)
    print("RUNNING ORIGINAL SELF-RAG BENCHMARK")
    print("="*50)
    
    test_queries = [
        "Leave odd one out: twitter, instagram, whatsapp.",
        "Can you tell me the difference between llamas and alpacas?",
        "What is the capital of France?",
        "Explain quantum computing."
    ]
    
    print("\n=== NO RETRIEVAL BENCHMARK ===")
    no_retrieval_results = benchmark_selfrag(model, test_queries)
    for result in no_retrieval_results:
        print(f"Query: {result['query']}")
        print(f"Response: {result['response']}")
        print(f"Retrieval triggered: {result['retrieval_used']}")
        print(f"Inference time: {result['inference_time']:.3f}s")
        print("-" * 50)
    
    print("\n=== WITH RETRIEVAL BENCHMARK ===")
    contexts = [
        "Social media platforms for sharing content.",
        "The alpaca (Lama pacos) is a species of South American camelid mammal. It is similar to, and often confused with, the llama. Alpacas are considerably smaller than llamas, and unlike llamas, they were not bred to be working animals, but were bred specifically for their fiber.",
        "Paris is the capital and most populous city of France.",
        "Quantum computing is a type of computation that harnesses quantum mechanics to process information in ways that classical computers cannot."
    ]
    
    with_retrieval_results = benchmark_selfrag(model, test_queries, contexts)
    for result in with_retrieval_results:
        print(f"Query: {result['query']}")
        print(f"Response: {result['response']}")
        print(f"Relevant: {result['is_relevant']}")
        print(f"Supported: {result['is_supported']}")
        print(f"Utility: {result['utility_score']}")
        print(f"Inference time: {result['inference_time']:.3f}s")
        print("-" * 50)
    
    # Save original benchmark results
    original_results = {
        'no_retrieval': no_retrieval_results,
        'with_retrieval': with_retrieval_results
    }
    
    # Menu for additional benchmarks
    print("\n" + "="*50)
    print("ADDITIONAL BENCHMARKS AVAILABLE")
    print("="*50)
    print("1. CRAG Benchmark (Task 1 & 2)")
    print("2. RAGBench - CovidQA")
    print("3. RAGBench - CUAD") 
    print("4. RAGBench - DelucionQA")
    print("5. All RAGBench datasets")
    print("6. Natural Questions")
    print("7. HotpotQA - Distractor")
    print("8. HotpotQA - FullWiki")
    print("9. All HotpotQA variants")
    print("10. SQuAD v2")
    print("11. Skip additional benchmarks")
    
    choice = input("Select additional benchmark (1-11, default 11): ").strip() or "11"
    
    additional_results = {}
    
    if choice == "1":
        sample_size = int(input("CRAG sample size (default 100): ") or "100")
        additional_results['crag'] = run_crag_benchmark(model, "1_and_2", sample_size)
    elif choice == "2":
        sample_size = int(input("RAGBench CovidQA sample size (default 100): ") or "100")
        additional_results['ragbench_covidqa'] = run_ragbench_benchmark(model, "covidqa", sample_size)
    elif choice == "3":
        sample_size = int(input("RAGBench CUAD sample size (default 100): ") or "100")
        additional_results['ragbench_cuad'] = run_ragbench_benchmark(model, "cuad", sample_size)
    elif choice == "4":
        sample_size = int(input("RAGBench DelucionQA sample size (default 100): ") or "100")
        additional_results['ragbench_delucionqa'] = run_ragbench_benchmark(model, "delucionqa", sample_size)
    elif choice == "5":
        sample_size = int(input("RAGBench sample size per dataset (default 100): ") or "100")
        additional_results['ragbench_all'] = run_all_ragbench_datasets(model, sample_size)
    elif choice == "6":
        sample_size = int(input("Natural Questions sample size (default 100): ") or "100")
        additional_results['natural_questions'] = run_natural_questions_benchmark(model, sample_size)
    elif choice == "7":
        sample_size = int(input("HotpotQA Distractor sample size (default 100): ") or "100")
        additional_results['hotpot_distractor'] = run_hotpot_qa_benchmark(model, "distractor", sample_size)
    elif choice == "8":
        sample_size = int(input("HotpotQA FullWiki sample size (default 100): ") or "100")
        additional_results['hotpot_fullwiki'] = run_hotpot_qa_benchmark(model, "fullwiki", sample_size)
    elif choice == "9":
        sample_size = int(input("HotpotQA sample size per variant (default 100): ") or "100")
        additional_results['hotpot_all'] = run_all_hotpot_variants(model, sample_size)
    elif choice == "10":
        sample_size = int(input("SQuAD v2 sample size (default 100): ") or "100")
        additional_results['squad_v2'] = run_squad_v2_benchmark(model, sample_size)
    
    # Combine all results
    all_results = {
        'original_benchmark': original_results,
        **additional_results
    }
    
    # Save results
    output_file = 'selfrag_benchmark_results.json'
    try:
        with open(output_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        print(f"\nBenchmark results saved to '{output_file}'")
    except Exception as e:
        print(f"Error saving results: {e}")
    
    print("Benchmark completed successfully!")

if __name__ == "__main__":
    main()
